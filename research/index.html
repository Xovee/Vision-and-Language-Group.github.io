<!DOCTYPE html>
<html>

  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7b6a73980f18da12874c3b3a7b9fda39";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>

<head>
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CFM(Song and Gao) - Research</title>
  <meta name="description" content="CMF(Song and Gao) -- Research.">
  <link rel="stylesheet" href="../css/main.css">
  <link rel="canonical" href="index.html">
<link rel="shortcut icon" type ="image/x-icon" href="../images/favicon.ico">
<style type="text/css">
  .navbar-background-container {
    background-image: url("../css/background.jpg");
    background-size: cover;
  }
</style>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82472331-1', 'auto');
  ga('send', 'pageview');

</script>


</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="navbar-background-container">
  <div class="container-fluid">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
    <span class="sr-only">Toggle navigation</span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    </button>
  
    <a class="navbar-brand" href="../index.html">Center for Future Media, UESTC</a>
  </div>
  <div class="collapse navbar-collapse" id="navbar-collapse-1">
    <ul class="nav navbar-nav navbar-right">
    <li><a href="../index.html">Home</a></li>
    <li><a href="index.html">Research</a></li>
    <li><a href="../team/index.html">Member</a></li>
    <li><a href="..publications/index.html">Publications</a></li>
<!--     <li><a href="../news/index.html">News</a></li> -->
    </ul>
  </div>
  </div>
</div>
</div>

<div class="container-fluid">
  <div class="row">
    <div id="textid" class="col-sm-12">
      <center><h1>Research</h1></center>
<!--       <h3>Overview of the research field</h3>
      
      <p>
        
      </p>
      <p>Our research interests mainly include multimedia analysis and computing, computer vision, machine learning, and artificial intelligence. Recently, we are focusing on the visual understanding via deep learning, e.g., video/image recognition, detection and segmentation, video/image captioning, and video/image question answering (QA). We also explore the deep learning methods’ vulnerability and its robustness to adversarial attacks. Our goal is to further understand the vulnerability and interpretability of deep learning methods, which will provide theoretic evidences and methodology strategies for constructing a safer and more reliable system of image semantic understanding.</p>
      <center><img src="overview.png" width="100%"></center> -->
<!-- <h1 id="funding">Funding</h1>

<ul>
  <li>The Key Theories and Methods for Intelligent Cross-media Question Answering and Reasoning, NSFC Key Program, Co-PI</li>
  <li>Collaboration Project with Huawei Noah’s Ark Lab on Machine Learning, PI</li>
  <li>The Underlying Theory Research and Visual Analysis Technology for Intelligent City Surveillance, NSFC Key Program, Co-PI</li>
  <li>Theories and Methods of Adversarial Machine Learning for Image Semantic Understanding, NSFC General Program, PI</li>
  <li>Ad Hoc Web Image Semantic Understanding with Limited Supervision, NSFC General Program, PI</li>
</ul> -->

<h3 id="work">Part of our achievement</h3>
<p><b> 1. Image retrival </b></p>
Our research interests are in image retrieval, hashing and vector quantization. 
<p><center><img src="https://i.loli.net/2019/09/22/jST1hmMRCquo9Vs.png" alt="" style="width: 600px;  border: 10px" /></center></p>
<p>The figure shows our Deep Recurrent Quantization (DRQ) model which is trained once and can generate sequential binary codes. It is the first to combine recurrent network and quantization, which significantly reduces the codebook size. The code length can be easily controlled by adjusting the number of recurrent iterations. Experimental results on the benchark datasets show that our model achieves comparable or even better performance compared with the state-of-the-arts for image retrieval. But it requires much less number of parameters and training times. This work is accepted as a conference paper in IJCAI-19. <a href="https://www.ijcai.org/proceedings/2019/128">[Paper link]</a>. <a href="https://github.com/cfm-uestc/DRQ">[Source code]</a>.</p> 
Other works:
<li>Deep Progressive Quantization (accepted in IJCAI-19): <a href="https://www.ijcai.org/proceedings/2019/102">[Paper link]</a>. <a href="https://github.com/cfm-uestc/DPQ">[Source code]</a>.</li>

<p><b> 2. Face Aging </b></p>
<p>Face aging and rejuvenation aims to predict the
face of a person at different ages. While tremendous progresses
have been made in this topic, there are two central problems
remaining largely unsolved:<li>the majority of prior works
require sequential training data, which is very rare in real
scenarios;</li> <li> how to simultaneously render aging face and
preserve personality.</li></p>
<p><center><img src="face.png" alt = "" style = "width:500px;  float: left;"/></center></p>
<p>To tackle these issues, we
develop a novel dual conditional GANs (AgeGAN) mechanism,
which enables face aging and rejuvenation to be trained from
multiple sets of unlabeled face images with different ages. In
our architecture, the primal conditional GAN transforms a face
image to other ages based on the age condition, while the
dual conditional GAN learns to invert the task. Hence a loss
function that accounts for the reconstruction error of images can
preserve the personal identity, while the discriminators on the
generated images learn the transition patterns (e.g., the shape and
texture changes between age groups) and guide the generation
of age-specific photo-realistic faces. </p>

<p>We further improve our
networks, termed AgeGAN++, in which we share the weights
between the primal part and the dual part to ensure a more
stable training process. Moreover, in order to get more sensible
results, a representation disentanglement component is integrated
with the latent facial representation, and a novel enhanced
discriminator is applied on the generated images of AgeGAN++.
Experimental results on two public datasets demonstrate the
appealing performance of the proposed methods by comparing
with the state-of-the-art methods. </p>


  <p><b> 3. Visual Question Answering </b></p>

<p>Visual Question Answering is that given an image or a video and a related question, the proposed algorithm can answer the question correctly based on the visual information. Most existing methods are mainly based on recurrent neural networks (RNNs) with attention. But they are time-consuming and having difficulties in modeling long range dependencies due to the sequential nature of RNNs. </p>
<p><center><img src="vqa.jpg" alt="" style="width:600px; border: 10px" /></center></p>
<p>To tackle this issue, we propose a new architecture, Positional Self-Attention with Co-Attention (PSAC), which does not require RNNs for video question answering. The proposed Positional Self-Attention Block can calculate the response at each position by attending to all positions within the same sequence and then add representations of absolute positions. Therefore, PSAC can exploit the global dependencies of question and temporal information in the video and make the process of question and video encoding executed in parallel.</p>
This <a href="http://staff.ustc.edu.cn/~hexn/papers/aaai19-VideoQA.pdf"> paper</a> is accepted as an oral paper by AAAI-2019.




<h3 id="work">Exploring</h3>
    <p><b> 1. Video object segmentation</b></p>

    <p>
      Video Object Segmentation is a task that separating a foreground object from a video sequence. We focus on semi-supervised VOS in which the ground truth segmentation masks of one or multiple objects are given for the first frame in a video. VOS is a fundamental task
in computer vision, with important applications including video editing, robotics, and self-driving cars. 
    </p>
            <p><center><img src="segmentation.jpg" alt="" style="width:600px; border: 10px" /></center></p>
    <p><b> 2. Visual-and-language Navigation </b></p>
    <p>    VLN requires an embodied agent to follow natural language instructions to navigate from a starting pose to a goal location in the Matterport3D Simulator(a new large-scale visual RL simulation environment).</p><br>
    <table border="0" align="center" cellpadding="0" cellspacing="0">
      <tr>
        <td valign="top">
        <img src="VLN.jpg" alt="" style="width: 300px;  border: 10px" /></td>
        <td valign="top">
        <img src="VLN2.jpg" alt="" style="width: 250px;  border: 10px" /></td>
        <td valign="top">
        <img src="VLN3.jpg" alt="" style="width: 300px;  border: 10px" /></td>
      </tr>
    </table>
    <p><b> 3. Adversarial Attack </b></p>
    <p><img src="FGSM.jpg" alt="" style="width: 450px; float: left; border: 10px" /></p>
    <p>Recent studies show that deep neural network are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. In the picture left, we can't see any difference between two images, but DNN really fail. For our group, It's really intersting to understand why it will happen. <br><i>Maybe you still feel it is a little boring, don't care it, let's see an interesting 
      <a href = "attack.mp4">video</a>. </i>This video is from paper "On Physical Adversarial Patches for Object Detection(arXiv 2019)".<br>
      There are also many other intersting application, explore it and make fun!
</p>
  </div>

</div>
</div>

<div id="footer" class="panel">
  <div class="panel-footer">
   <div class="container-fluid">
     <div class="row">
      <center>
        <div class="col-sm-12">
         Contact: Room A305, innovation center, University of Electronic Science and Technology of China (<a href="https://www.amap.com/search?query=%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E6%B8%85%E6%B0%B4%E6%B2%B3%E6%A0%A1%E5%8C%BA%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83&city=510100&geoobj=103.851502%7C30.542145%7C104.389832%7C30.782536&zoom=12">Maps</a>) <br />
                   If there are any bug, please contact <a href="mailto:qilong.zhangdl@gmail.com">qilong.zhangdl@gmail.com</a>. Thank you!
          (2019-9-25) <br />
          <!-- <span id="busuanzi_container_site_pv">本站总访问量：<span id="busuanzi_value_site_pv"></span>次</span> -->
        </center>
      </div>
    </div>
  </div>
</div>
</div>



<script src="../js/jquery.min.js"></script>
<script src="../js/bootstrap.min.js"></script>


</body>

</html>
