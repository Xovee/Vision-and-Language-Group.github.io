<!DOCTYPE html>
<html>

<script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?7b6a73980f18da12874c3b3a7b9fda39";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
</script>

<head>
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CMF(Song and Gao) - Research</title>
  <meta name="description" content="CMF(Song and Gao) -- Research">
  <link rel="stylesheet" href="../css/main.css">
  <link rel="canonical" href="index.html">
  <link rel="shortcut icon" type ="image/x-icon" href="../images/favicon.ico">


  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-82472331-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


<body>

  <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container-fluid">
     <div class="navbar-header">
       <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <a class="navbar-brand" href="../index.html">Center of Future Media, UESTC</a>
    </div>
    <div class="collapse navbar-collapse" id="navbar-collapse-1">
     <ul class="nav navbar-nav navbar-right">
      <li><a href="../index.html">Home</a></li>
      <li><a href="index.html">Research</a></li>
      <li><a href="../team/index.html">Member</a></li>
      <li><a href="../publications/index.html">Publications</a></li>
<!--       <li><a href="../news/index.html">News</a></li> -->
      <!--<li><a href="/vacancies">Join us</a></li>-->
      <!--<li><a href="/pictures">(Pics)</a></li>-->
    </ul>
  </div>
</div>
</div>

<div class="container-fluid">
  <div class="row">
    <div id="textid" class="col-sm-12">
      <center><bold><h1>Visual Understanding</h1></bold></center>
      <h2>Overview of the research field</h2>
      
      <p>
        待输入
      </p>
      <!-- <p>Our research interests mainly include multimedia analysis and computing, computer vision, machine learning, and artificial intelligence. Recently, we are focusing on the visual understanding via deep learning, e.g., video/image recognition, detection and segmentation, video/image captioning, and video/image question answering (QA). We also explore the deep learning methods’ vulnerability and its robustness to adversarial attacks. Our goal is to further understand the vulnerability and interpretability of deep learning methods, which will provide theoretic evidences and methodology strategies for constructing a safer and more reliable system of image semantic understanding.</p> -->
      <center><img src="overview.png" width="100%"></center>
<!-- <h1 id="funding">Funding</h1>

<ul>
  <li>The Key Theories and Methods for Intelligent Cross-media Question Answering and Reasoning, NSFC Key Program, Co-PI</li>
  <li>Collaboration Project with Huawei Noah’s Ark Lab on Machine Learning, PI</li>
  <li>The Underlying Theory Research and Visual Analysis Technology for Intelligent City Surveillance, NSFC Key Program, Co-PI</li>
  <li>Theories and Methods of Adversarial Machine Learning for Image Semantic Understanding, NSFC General Program, PI</li>
  <li>Ad Hoc Web Image Semantic Understanding with Limited Supervision, NSFC General Program, PI</li>
</ul> -->

<h2 id="work">Part of our work</h2>
<p><b> 1. Hash </b></p>
<p><b> 2. Face Aging </b></p>
<!-- <p><img src="face.png" alt="" style="width: 350px; float: right; border: 10px" /></p> -->
<p><center><img src="face.png" width="70%"/></center></p>


  <p><b> 3. Visual Question Answering </b></p>

  <p>VQA is a task that given a picture or video and a corresponding natural language question, model can generate the answer automatically. It is a multi-media task which combines the technique of Natural Language Processing(NLP) and Computer Vision(CV). It has received much attention from researchers and scholars in recent years. Solving this task is a crucial step towards Artificial Intelligence.</p>

  <p><img src="https://tjumm.github.io/images/respic/qa.jpg" alt="" style="width: 300px; float: left; border: 10px" /></p>

  <p>VQA can be divided into two categories in terms of the visual content: Image Question Answering(ImageQA) and Video Question Answering(VideoQA). As the natural extension of images, videos contain richer information with additional temporal and dynamic characteristics, which brings up more challenges and difficulties. The main aspect that our lab focuses on is how to combine the reasoning and knowledge to solve the VideoQA. When answering the complex question, human usually reason about the visual and language or combine the additional knowledge. So if we can let models to achieve such abilities, they will more intelligent.</p>

  <p>The existing achievements of this topic of our lab include:
    <i>“Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents” (AAAI).</i>This work contributes a Layered Memory Network (LMN) to solve the MovieQA, which represents frame-level and clip-level movie content by the Static Word Memory module and the Dynamic Subtitle Memory module, respectively.
    <i>“Explore Multi-step Reasoning in Video Question Answering” (ACMMM). </i>This work explores multi-step reasoning in VideoQA by formulating it as a new task, which targets to answer compositional logical structured questions bases on videos. The work contributes in developing a system to automatically generate a large-scale VideoQA dataset and proposing a novel model which combines spatial and temporal attention to address this task.</p>




<h2 id="work">Exploring</h2>
    <p><b> 1. Video object segmentation</b></p>
    <p><b> 2. Visual-and-language Navigation </b></p>
    <p><b> 3. Adversarial Machine Learning </b></p>
    <p><img src="https://tjumm.github.io/images/respic/adv.jpg" alt="" style="width: 350px; float: right; border: 10px" /></p>

  </div>

</div>
</div>

<div id="footer" class="panel">
  <div class="panel-footer">
   <div class="container-fluid">
     <div class="row">
      <center>
        <div class="col-sm-12">
         Contact: Room A305, innovation center, University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/">Maps</a>) <br />
            2019-9-25 <br />
          <!-- <span id="busuanzi_container_site_pv">本站总访问量：<span id="busuanzi_value_site_pv"></span>次</span> -->
        </center>
      </div>
    </div>
  </div>
</div>
</div>



<script src="../js/jquery.min.js"></script>
<script src="../js/bootstrap.min.js"></script>


</body>

</html>
